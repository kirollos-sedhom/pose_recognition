{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "with open('x_train.pkl', 'rb') as file:\n",
    "    x_train = pickle.load(file)\n",
    "\n",
    "with open('y_train.pkl', 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "with open('x_test.pkl', 'rb') as file:\n",
    "    x_test = pickle.load(file)\n",
    "\n",
    "with open('y_test.pkl', 'rb') as file:\n",
    "    y_test = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # cnn takes input of shape (batch_size, channels, seq_len)\n",
    "        # print(\"x equals: \",x)\n",
    "        # x = x[0][0][0]\n",
    "        # x = torch.tensor(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out = self.cnn(x)\n",
    "        # lstm takes input of shape (batch_size, seq_len, input_size)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Assuming x_train is a list of tensors with variable lengths\n",
    "# Convert x_train to a list of torch tensors\n",
    "\n",
    "x_train = [torch.tensor(row, dtype=torch.float32) for row in x_train]\n",
    "\n",
    "# Pad sequences to make them the same length\n",
    "\n",
    "x_train = pad_sequence(x_train, batch_first=True, padding_value=0)\n",
    "\n",
    "# Now x_train_padded contains the padded sequences with the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[0.2747, 0.9711, 0.1519],\n",
       "            [0.2654, 0.9813, 0.1329],\n",
       "            [0.2694, 0.9606, 0.0967],\n",
       "            ...,\n",
       "            [0.5255, 0.7296, 0.1994],\n",
       "            [0.7167, 0.6448, 0.3350],\n",
       "            [0.7217, 0.6438, 0.5097]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.2810, 0.9666, 0.2426],\n",
       "            [0.2653, 0.9803, 0.1755],\n",
       "            [0.2703, 0.9587, 0.2249],\n",
       "            ...,\n",
       "            [0.5628, 0.6424, 0.1664],\n",
       "            [0.7190, 0.6422, 0.2386],\n",
       "            [0.7252, 0.6391, 0.5088]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.2779, 0.9705, 0.2488],\n",
       "            [0.2625, 0.9835, 0.1559],\n",
       "            [0.2644, 0.9656, 0.2193],\n",
       "            ...,\n",
       "            [0.4755, 0.7968, 0.1343],\n",
       "            [0.6516, 0.8479, 0.0384],\n",
       "            [0.7282, 0.6428, 0.5025]]]],\n",
       "\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1707, 0.3383, 0.2192],\n",
       "            [0.1525, 0.3526, 0.4098],\n",
       "            [0.1590, 0.3270, 0.3003],\n",
       "            ...,\n",
       "            [0.5264, 0.2949, 0.4614],\n",
       "            [0.6274, 0.4258, 0.1915],\n",
       "            [0.6277, 0.3161, 0.3407]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1731, 0.3338, 0.1933],\n",
       "            [0.1614, 0.3478, 0.2726],\n",
       "            [0.1671, 0.3248, 0.1648],\n",
       "            ...,\n",
       "            [0.5365, 0.3757, 0.2683],\n",
       "            [0.6428, 0.4513, 0.2131],\n",
       "            [0.6398, 0.4369, 0.1731]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1830, 0.3313, 0.2937],\n",
       "            [0.1706, 0.3455, 0.3276],\n",
       "            [0.1738, 0.3242, 0.2421],\n",
       "            ...,\n",
       "            [0.5339, 0.2937, 0.4980],\n",
       "            [0.6440, 0.4425, 0.1377],\n",
       "            [0.6354, 0.3143, 0.3255]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[0.6357, 0.7531, 0.1553],\n",
       "            [0.5672, 0.6997, 0.0860],\n",
       "            [0.5676, 0.6844, 0.1101],\n",
       "            ...,\n",
       "            [0.5379, 0.3975, 0.2750],\n",
       "            [0.7451, 0.6458, 0.4979],\n",
       "            [0.7431, 0.6575, 0.4644]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.4880, 0.7703, 0.0576],\n",
       "            [0.1368, 0.3720, 0.0370],\n",
       "            [0.5226, 0.6944, 0.0520],\n",
       "            ...,\n",
       "            [0.5269, 0.3930, 0.3198],\n",
       "            [0.7391, 0.6467, 0.4994],\n",
       "            [0.7369, 0.6611, 0.4201]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1901, 0.3306, 0.0737],\n",
       "            [0.1611, 0.3475, 0.0778],\n",
       "            [0.1649, 0.3323, 0.0788],\n",
       "            ...,\n",
       "            [0.5225, 0.3929, 0.3347],\n",
       "            [0.7388, 0.6463, 0.5436],\n",
       "            [0.7357, 0.6601, 0.4608]]]],\n",
       "\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1548, 0.3879, 0.2556],\n",
       "            [0.1387, 0.3944, 0.1899],\n",
       "            [0.1404, 0.3794, 0.2334],\n",
       "            ...,\n",
       "            [0.5533, 0.4574, 0.1744],\n",
       "            [0.6491, 0.4727, 0.3564],\n",
       "            [0.6524, 0.4954, 0.1830]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1639, 0.3887, 0.2532],\n",
       "            [0.1512, 0.3932, 0.2345],\n",
       "            [0.1530, 0.3778, 0.2174],\n",
       "            ...,\n",
       "            [0.5644, 0.4501, 0.2418],\n",
       "            [0.6479, 0.4780, 0.4449],\n",
       "            [0.6562, 0.4886, 0.3328]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1731, 0.3994, 0.3519],\n",
       "            [0.1664, 0.4080, 0.3823],\n",
       "            [0.1644, 0.3872, 0.3185],\n",
       "            ...,\n",
       "            [0.5605, 0.4498, 0.2337],\n",
       "            [0.6513, 0.4873, 0.3099],\n",
       "            [0.6587, 0.4968, 0.3126]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[0.1731, 0.3528, 0.1192],\n",
       "            [0.1655, 0.3633, 0.1570],\n",
       "            [0.1661, 0.3392, 0.2004],\n",
       "            ...,\n",
       "            [0.4663, 0.3101, 0.3617],\n",
       "            [0.5706, 0.3333, 0.3286],\n",
       "            [0.5731, 0.3034, 0.3026]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1718, 0.2946, 0.1631],\n",
       "            [0.1602, 0.2916, 0.1645],\n",
       "            [0.1608, 0.2829, 0.1759],\n",
       "            ...,\n",
       "            [0.5539, 0.6677, 0.1445],\n",
       "            [0.7090, 0.6557, 0.5602],\n",
       "            [0.6176, 0.7388, 0.3398]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1613, 0.3657, 0.1501],\n",
       "            [0.1503, 0.3531, 0.1054],\n",
       "            [0.1511, 0.3390, 0.1360],\n",
       "            ...,\n",
       "            [0.4559, 0.3191, 0.3582],\n",
       "            [0.5612, 0.3221, 0.2904],\n",
       "            [0.5655, 0.3155, 0.3516]]]],\n",
       "\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1794, 0.2898, 0.1055],\n",
       "            [0.1746, 0.2969, 0.1251],\n",
       "            [0.1771, 0.2897, 0.1079],\n",
       "            ...,\n",
       "            [0.4913, 0.6431, 0.2547],\n",
       "            [0.6388, 0.5744, 0.4490],\n",
       "            [0.5674, 0.6797, 0.3987]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1893, 0.2873, 0.1562],\n",
       "            [0.1807, 0.2843, 0.1419],\n",
       "            [0.1841, 0.2826, 0.1593],\n",
       "            ...,\n",
       "            [0.4962, 0.6168, 0.4053],\n",
       "            [0.6284, 0.5757, 0.2775],\n",
       "            [0.5660, 0.6767, 0.3932]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.1854, 0.2864, 0.1511],\n",
       "            [0.1786, 0.2829, 0.1356],\n",
       "            [0.1812, 0.2831, 0.1440],\n",
       "            ...,\n",
       "            [0.5031, 0.6176, 0.4261],\n",
       "            [0.6513, 0.5749, 0.4539],\n",
       "            [0.6520, 0.5858, 0.4480]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[0.6277, 0.6574, 0.3464],\n",
       "            [0.6123, 0.6738, 0.2679],\n",
       "            [0.6204, 0.6599, 0.3043],\n",
       "            ...,\n",
       "            [0.6589, 0.2566, 0.5363],\n",
       "            [0.6676, 0.1403, 0.5623],\n",
       "            [0.6856, 0.1198, 0.7464]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6249, 0.6598, 0.2963],\n",
       "            [0.6138, 0.6758, 0.2487],\n",
       "            [0.6215, 0.6639, 0.2781],\n",
       "            ...,\n",
       "            [0.6605, 0.2469, 0.4806],\n",
       "            [0.6697, 0.1391, 0.5769],\n",
       "            [0.6878, 0.1176, 0.7713]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6257, 0.6666, 0.3041],\n",
       "            [0.6139, 0.6818, 0.2396],\n",
       "            [0.6211, 0.6709, 0.2899],\n",
       "            ...,\n",
       "            [0.6628, 0.2486, 0.5868],\n",
       "            [0.6691, 0.1317, 0.6601],\n",
       "            [0.6885, 0.1132, 0.7443]]]],\n",
       "\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6857, 0.5995, 0.4265],\n",
       "            [0.7011, 0.6149, 0.3509],\n",
       "            [0.6906, 0.6133, 0.3353],\n",
       "            ...,\n",
       "            [0.6790, 0.2829, 0.1865],\n",
       "            [0.6891, 0.2124, 0.2185],\n",
       "            [0.6851, 0.2156, 0.2258]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6813, 0.5945, 0.4461],\n",
       "            [0.6909, 0.6044, 0.3536],\n",
       "            [0.6823, 0.6037, 0.2930],\n",
       "            ...,\n",
       "            [0.6819, 0.2957, 0.1969],\n",
       "            [0.6862, 0.2038, 0.1845],\n",
       "            [0.6814, 0.2198, 0.1821]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6890, 0.5854, 0.4343],\n",
       "            [0.6985, 0.6025, 0.3833],\n",
       "            [0.6892, 0.5963, 0.5043],\n",
       "            ...,\n",
       "            [0.6763, 0.2599, 0.1742],\n",
       "            [0.6899, 0.1548, 0.2572],\n",
       "            [0.6822, 0.2071, 0.2106]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[0.6274, 0.6515, 0.3167],\n",
       "            [0.6200, 0.6610, 0.2662],\n",
       "            [0.6215, 0.6539, 0.2987],\n",
       "            ...,\n",
       "            [0.6541, 0.2434, 0.4943],\n",
       "            [0.6606, 0.1289, 0.6085],\n",
       "            [0.6735, 0.1045, 0.7399]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6197, 0.6584, 0.2988],\n",
       "            [0.6059, 0.6720, 0.3248],\n",
       "            [0.6133, 0.6596, 0.2682],\n",
       "            ...,\n",
       "            [0.6594, 0.2406, 0.5157],\n",
       "            [0.6590, 0.1305, 0.6243],\n",
       "            [0.6728, 0.1031, 0.7101]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6228, 0.6596, 0.3412],\n",
       "            [0.6105, 0.6727, 0.3115],\n",
       "            [0.6136, 0.6629, 0.3108],\n",
       "            ...,\n",
       "            [0.6577, 0.2402, 0.4889],\n",
       "            [0.6572, 0.1313, 0.6253],\n",
       "            [0.6736, 0.1060, 0.7204]]]],\n",
       "\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.7083, 0.6250, 0.3171],\n",
       "            [0.6968, 0.6240, 0.3445],\n",
       "            [0.6928, 0.6185, 0.3578],\n",
       "            ...,\n",
       "            [0.6843, 0.2814, 0.2781],\n",
       "            [0.6662, 0.1860, 0.3104],\n",
       "            [0.6672, 0.1443, 0.5463]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6934, 0.6123, 0.3439],\n",
       "            [0.6882, 0.6177, 0.3696],\n",
       "            [0.6875, 0.6075, 0.4849],\n",
       "            ...,\n",
       "            [0.6830, 0.3037, 0.2968],\n",
       "            [0.6660, 0.1441, 0.4453],\n",
       "            [0.6696, 0.1494, 0.4775]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6971, 0.6089, 0.3793],\n",
       "            [0.6912, 0.6149, 0.3987],\n",
       "            [0.6857, 0.6043, 0.3669],\n",
       "            ...,\n",
       "            [0.6818, 0.2988, 0.2695],\n",
       "            [0.6668, 0.1427, 0.4265],\n",
       "            [0.6711, 0.1450, 0.5349]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[0.6254, 0.6438, 0.4283],\n",
       "            [0.6156, 0.6543, 0.4094],\n",
       "            [0.6171, 0.6515, 0.3744],\n",
       "            ...,\n",
       "            [0.6266, 0.2589, 0.3375],\n",
       "            [0.5952, 0.1350, 0.5519],\n",
       "            [0.6066, 0.1193, 0.7073]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6308, 0.6387, 0.4867],\n",
       "            [0.6182, 0.6482, 0.3329],\n",
       "            [0.6222, 0.6467, 0.4131],\n",
       "            ...,\n",
       "            [0.6274, 0.2685, 0.3604],\n",
       "            [0.5924, 0.1349, 0.5172],\n",
       "            [0.6022, 0.1198, 0.6781]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.6290, 0.6379, 0.4207],\n",
       "            [0.6203, 0.6489, 0.3066],\n",
       "            [0.6212, 0.6462, 0.3601],\n",
       "            ...,\n",
       "            [0.6352, 0.2620, 0.3826],\n",
       "            [0.5981, 0.1348, 0.5743],\n",
       "            [0.6078, 0.1183, 0.6197]]]],\n",
       "\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.7375, 0.6408, 0.3359],\n",
       "            [0.7310, 0.6482, 0.3630],\n",
       "            [0.7295, 0.6412, 0.3646],\n",
       "            ...,\n",
       "            [0.6692, 0.2690, 0.4116],\n",
       "            [0.6118, 0.1562, 0.5311],\n",
       "            [0.6204, 0.1385, 0.5859]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.7423, 0.6456, 0.3728],\n",
       "            [0.7379, 0.6527, 0.3750],\n",
       "            [0.7343, 0.6489, 0.2948],\n",
       "            ...,\n",
       "            [0.6684, 0.2695, 0.3739],\n",
       "            [0.6128, 0.1561, 0.5264],\n",
       "            [0.6217, 0.1382, 0.5648]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0.7476, 0.6455, 0.4305],\n",
       "            [0.7424, 0.6536, 0.3537],\n",
       "            [0.7390, 0.6491, 0.3503],\n",
       "            ...,\n",
       "            [0.6684, 0.2708, 0.4199],\n",
       "            [0.6145, 0.1574, 0.5463],\n",
       "            [0.6225, 0.1395, 0.6122]]]]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 3  # Set your input size\n",
    "hidden_size = 3  # Set your hidden size\n",
    "num_layers = 5  # Set your number of layers\n",
    "num_classes = len(np.unique(y_train))  # Set your number of classes\n",
    "model = CNN_LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 5  # Set your number of epochs\n",
    "batch_size = 5  # Set your batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 6 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m y_train[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\kirol\\OneDrive\\Documents\\repos\\GP_PRACTICE\\GP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kirol\\OneDrive\\Documents\\repos\\GP_PRACTICE\\GP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[71], line 20\u001b[0m, in \u001b[0;36mCNN_LSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# cnn takes input of shape (batch_size, channels, seq_len)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# print(\"x equals: \",x)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# x = x[0][0][0]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# x = torch.tensor(x)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(x)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# lstm takes input of shape (batch_size, seq_len, input_size)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 6 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        inputs = x_train[i:i + batch_size]\n",
    "        labels = y_train[i:i + batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(x_train) // batch_size}], Loss: {loss.item()}')\n",
    "\n",
    "print('Training finished.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
